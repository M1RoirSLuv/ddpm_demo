import torch
import os
import sys
import numpy as np
from PIL import Image
from tqdm import tqdm
from omegaconf import OmegaConf

# 1. è‡ªåŠ¨å…³è”æœ¬åœ° ldm åº“è·¯å¾„
# å¦‚æœä½ çš„ ldm æ–‡ä»¶å¤¹åœ¨å½“å‰ç›®å½•ä¸‹ï¼Œè¿™è¡Œæ˜¯å¿…é¡»çš„
sys.path.append(os.getcwd())
from ldm.util import instantiate_from_config

def get_vae(config_path, ckpt_path):
    print("ğŸš€ æ­£åœ¨åŠ è½½ VAE (FP16 æ¨¡å¼)...")
    config = OmegaConf.load(config_path)
    
    # ã€æ ¸å¿ƒä¿®æ”¹ 1ã€‘å±è”½ LPIPS è”ç½‘æ£€æŸ¥
    config.model.params.lossconfig.target = "torch.nn.Identity"
    
    # å®ä¾‹åŒ–æ¨¡å‹
    model = instantiate_from_config(config.model)
    
    # ã€æ ¸å¿ƒä¿®æ”¹ 2ã€‘è§£å†³ PyTorch 2.6+ çš„æƒé‡åŠ è½½å®‰å…¨é™åˆ¶ (weights_only=False)
    print(f"ğŸ“‚ è¯»å–æƒé‡æ–‡ä»¶: {ckpt_path}")
    sd = torch.load(ckpt_path, map_location="cpu", weights_only=False)["state_dict"]
    model.load_state_dict(sd, strict=False)
    
    # ã€æ ¸å¿ƒä¿®æ”¹ 3ã€‘å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦ (FP16) å¹¶ç§»è‡³ GPU
    model = model.cuda().half().eval()
    return model

@torch.no_grad()
def main():
    # --- é…ç½®åŒºåŸŸ ---
    IMG_DIR = "./data/raw_images"       # ä½ çš„ 1280x1024 å›¾ç‰‡å­˜æ”¾æ–‡ä»¶å¤¹
    SAVE_DIR = "./data/latents"         # ç‰¹å¾å‘é‡ä¿å­˜æ–‡ä»¶å¤¹
    VAE_CONFIG = "vae_config.yaml"
    VAE_CKPT = "model/autoencoder.ckpt"
    # ----------------
    
    os.makedirs(SAVE_DIR, exist_ok=True)
    
    try:
        vae = get_vae(VAE_CONFIG, VAE_CKPT)
    except Exception as e:
        print(f"âŒ åŠ è½½ VAE å¤±è´¥: {e}")
        return
    
    img_files = [f for f in os.listdir(IMG_DIR) if f.endswith(('.png', '.jpg', '.jpeg'))]
    print(f"æ‰¾åˆ° {len(img_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹æå–ç‰¹å¾...")

    for fname in tqdm(img_files):
        try:
            # 1. åŠ è½½å¹¶å¼ºåˆ¶è½¬ä¸º RGB (3é€šé“)
            img_path = os.path.join(IMG_DIR, fname)
            img = Image.open(img_path).convert("RGB")
            img = img.resize((1280, 1024)) 
            
            # 2. å½’ä¸€åŒ–å¹¶è½¬ä¸º FP16 Tensor
            img_np = np.array(img).astype(np.float32) / 127.5 - 1.0
            # æ³¨æ„ï¼šæ•°æ®ä¹Ÿè¦è½¬æˆ .half() æ‰èƒ½ä¸æ¨¡å‹åŒ¹é…
            img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0).cuda().half()
            
            # 3. æ‰§è¡Œç¼–ç 
            # VQModel ç¼–ç è¿”å› quant (ç‰¹å¾), emb_loss, info
            latent, _, _ = vae.encode(img_tensor)
            
            # 4. ä¿å­˜ Latent (ä¿å­˜æ—¶è½¬å› FP32 å¯ä»¥é¿å…ç²¾åº¦ç´¯ç§¯è¯¯å·®ï¼Œä¸”å ç”¨ç©ºé—´å˜åŒ–ä¸å¤§)
            save_path = os.path.join(SAVE_DIR, os.path.splitext(fname)[0] + ".pt")
            torch.save(latent.squeeze(0).float().cpu(), save_path)
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å›¾ç‰‡ {fname} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue

    print(f"\nâœ… é¢„å¤„ç†å®Œæˆï¼Latent æ–‡ä»¶å·²ä¿å­˜åœ¨: {SAVE_DIR}")

if __name__ == "__main__":
    # å»ºè®®åœ¨è¿è¡Œå‰è®¾ç½®æ­¤ç¯å¢ƒå˜é‡ä»¥å‡å°‘æ˜¾å­˜ç¢ç‰‡
    os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
    main()